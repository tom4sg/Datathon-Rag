# -*- coding: utf-8 -*-
"""Datathon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cn541LlG-6TOJt60FTZtIb_WqrnyFrkh

# Datathon 2025 RAG Solution

### Ingestion

Let's first import relevant packages, take a look at our data, and create a master dataframe.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

media_df = pd.read_csv('P2/media.csv')
places_p2_df = pd.read_csv('P2/places.csv')
reviews_df = pd.read_csv('P2/reviews.csv')

print(f"P2 Media preview: {media_df.head}\n")
print(f"P2 Places preview: {places_p2_df.head}\n")
print(f"P2 Reviews preview: {reviews_df.head}\n")

"""Aggregate reviews corresponding to a given place_id into a list of strings. Do the same for media, then merge both into larger df."""

reviews_df = reviews_df.drop_duplicates(subset=['place_id', 'review_text'])
media_df = media_df.drop_duplicates(subset=['place_id', 'media_url'])

reviews_agg = (
    reviews_df
    .groupby('place_id')['review_text']
    .apply(list)
    .reset_index()
)

media_agg = (
    media_df
    .groupby('place_id')['media_url']
    .apply(list)
    .reset_index()
)

merge_df = pd.merge(places_p2_df, media_agg, on='place_id', how='inner')
merge_df = pd.merge(merge_df, reviews_agg, on='place_id', how='inner')
merge_df = merge_df.drop_duplicates(subset='place_id').reset_index(drop=True)
merge_df.head()

"""Let's concatenate reviews associated with a given place_id in preparation for embedding. Hopefully this can help with vibier searches."""

def concat_reviews(series):
    """Join all review texts in the group into a single string."""
    return ' '.join(series.astype(str))

# Group reviews and create the all_reviews column
agg_reviews = (
    reviews_df
    .groupby('place_id')['review_text']
    .apply(concat_reviews)
    .reset_index(name='concat_reviews')
)

# Merge the concatenated reviews into merge_df
merge_df = merge_df.merge(
    agg_reviews,
    how='left',
    left_on='place_id',
    right_on='place_id'
)

"""Let's write a function to prepare available structured data for semantic embedding, tack on the concatenated reviews and add it to our df."""

def combining_text(row):
    name = str(row.get('name', ''))
    neighborhood = str(row.get('neighborhood', ''))
    tags = str(row.get('tags', ''))
    short_description = str(row.get('short_description', ''))
    emojis = str(row.get('emojis', ''))
    reviews = str(row.get('concat_reviews', ''))

    combined_text = (
        f"Name: {name}. "
        f"Neighborhood: {neighborhood}. "
        f"Tags: {tags}. "
        f"Description: {short_description}. "
        f"Emojis: {emojis}."
        f"User Reviews: {reviews}."
    )
    return combined_text

merge_df['combined_text'] = merge_df.apply(combining_text, axis=1)

"""Ok, now let's actually start embedding our data. Cross-Encoders will be too computationally heavy for this. Instead will use bi-encoders such as sentence_transformers and CLIP for seemantic and multimodal embeddings respectively.

### Metadata Text Embeddings (Dense + Sparse)

Eventually, we want a hybrid search which requires is a combination of dense, sparse, and multimodal embeddings. Let's start with dense.
"""

# Load tqdm and MiniLM model(dense semantic text embedding)
from tqdm.auto import tqdm
from sentence_transformers import SentenceTransformer
metadata_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

embeddings = []
batch_size = 32

for i in tqdm(range(0, len(merge_df), batch_size), desc="Generating embeddings from dataset"):
    batch = merge_df['combined_text'].iloc[i: i + batch_size].tolist()
    batch_embeddings = metadata_model.encode(batch, normalize_embeddings=True)
    embeddings.append(batch_embeddings)

embeddings = np.vstack(embeddings)

# Save in Dataframe
merge_df['dense_metadata_embedding'] = embeddings.tolist()

"""Ok, now let's try a sparse text embedding model from fastembed. Hopefully with the hybrid, we can pick up explicit meaning as well as implied."""

!pip install fastembed

from fastembed import SparseTextEmbedding
sparse_model = SparseTextEmbedding(model_name="prithivida/Splade_PP_en_v1")

"""Time to embed!"""

sparse_embeddings = []

for i in tqdm(range(0, len(merge_df), batch_size), desc="Generating sparse embeddings"):
    batch = merge_df['combined_text'].iloc[i: i + batch_size].tolist()
    batch_embeddings = list(sparse_model.embed(batch))
    sparse_embeddings.extend(batch_embeddings)

merge_df['sparse_metadata_embedding'] = sparse_embeddings

"""Let's save it real quick."""

import joblib

# Save the DataFrame
joblib.dump(merge_df, 'merge_dense_and_sparse_df.joblib')

"""### Media (Image) Embeddings

Because there are over 30,000 media_urls, We opted for batch processing 1 image per place_id, reducing the image's resolution, and then embedding it using a Hugging Face CLIP model for multimodal embedding in another notebook. The result is the image_embeddings_sorted.csv that we can simply read in as a df. Ideally, with more time we could embed several/all images corresponding to a place_id, and then take their arithmetic mean for a more generally representative embedding per place_id.
"""

image_df = pd.read_csv('image_embeddings_sorted.csv')

"""Also, even though we already embedded the images in another file, let's import and load the CLIP model hear to use for embedding queries in the future."""

# Embedded 1 image for each location using CLIPProcessor, CLIPModel in another ipynb
from transformers import CLIPProcessor, CLIPModel

# Load the CLIP model and processor
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")

"""### Vector Similarity Search + Hybrid Search Model

Let's use Facebook AI Similarity Search (FAISS) for efficient vector similarity search on our:
- Metadata Embeddings (Dense)
- Media Embeddings (Image)

And, let's use Cosine Similarity for our sparse model.
"""

!pip install faiss-cpu

import faiss
from sklearn.metrics.pairwise import cosine_similarity

"""Metadata FAISS (Dense)"""

# Get metadata embeddings
all_dense_embeddings = np.array(merge_df['dense_metadata_embedding'].tolist(), dtype='float32')

# Determine embedding dimension
d = all_dense_embeddings.shape[1]

# Create FAISS index (L2 distance)
index_dense_metadata = faiss.IndexFlatL2(d)
index_dense_metadata.add(all_dense_embeddings)

print(f"Built FAISS dense metadata index with {index_dense_metadata.ntotal} vectors of dimension {d}.")

# Let's make a search function
def search_places_dense_metadata(query, index=index_dense_metadata, top_k=5):
    query_embedding = metadata_model.encode([query], normalize_embeddings=True)[0].astype('float32').reshape(1, -1)
    distances, indices = index.search(query_embedding, top_k)

    results = []
    for i, idx in enumerate(indices[0]):
        row = merge_df.iloc[idx]
        results.append({
            'Place Name': row['name'],
            'Neighborhood': row['neighborhood'],
            'Tags': row['tags'],
            'Description': row['short_description'],
            'Distance': distances[0][i]
        })
    return results

# Let's try a query and output the 5 nearest embeddings
query = "where to drink a matcha"
search_results = search_places_dense_metadata(query, index_dense_metadata, top_k=5)

for result in search_results:
    print("------------------------------")
    print(f"Place Name   : {result['Place Name']}")
    print(f"Neighborhood : {result['Neighborhood']}")
    print(f"Tags         : {result['Tags']}")
    print(f"Description  : {result['Description']}")
    print(f"L2 Distance  : {result['Distance']:.4f}")

"""Image FAISS"""

# Extract embedding columns (assumes embed_0 to embed_511)
embedding_cols = [col for col in image_df.columns if col.startswith('embed_')]
all_embeddings = image_df[embedding_cols].to_numpy().astype('float32')

# Determine embedding dimension
d = all_embeddings.shape[1]

# Build FAISS index
index_image = faiss.IndexFlatL2(d)
index_image.add(all_embeddings)

print(f"Built FAISS image index with {index_image.ntotal} vectors of dimension {d}.")

# Let's make a search function
def search_places_image(query, index=index_image, top_k=5):
    # Encode the text query using CLIP text encoder
    inputs = processor(text=[query], return_tensors="pt", padding=True)
    query_embedding = clip_model.get_text_features(**inputs)
    query_embedding = query_embedding / query_embedding.norm(p=2, dim=-1, keepdim=True)
    query_embedding = query_embedding.detach().cpu().numpy().astype('float32').reshape(1, -1)

    # Perform FAISS search
    distances, indices = index.search(query_embedding, top_k)

    # Gather results from image_df (which holds the place info)
    results = []
    for i, idx in enumerate(indices[0]):
        row = merge_df.iloc[idx]
        results.append({
            'Place Name': row['name'],
            'Neighborhood': row['neighborhood'],
            'Tags': row['tags'],
            'Description': row['short_description'],
            'Distance': distances[0][i]
        })
    return results

# Let's try a query and output the 5 nearest embeddings
search_results = search_places_image("Something to do on a gloomy day", index_image, top_k=5)

for result in search_results:
    print("------------------------------")
    print(f"Place Name   : {result['Place Name']}")
    print(f"Neighborhood : {result['Neighborhood']}")
    print(f"Tags         : {result['Tags']}")
    print(f"Description  : {result['Description']}")
    print(f"L2 Distance  : {result['Distance']:.4f}")

"""Metadata Cosine Similarity (Sparse)"""

def sparse_to_dense(sparse_embedding, dim=30315):
    dense_vec = np.zeros(dim, dtype=np.float32)
    dense_vec[sparse_embedding.indices] = sparse_embedding.values
    return dense_vec

print("Converting sparse embeddings to dense matrix...")
sparse_embeddings_dense = np.vstack([
    sparse_to_dense(embedding, dim=30315)
    for embedding in tqdm(merge_df['sparse_metadata_embedding'], desc="Converting embeddings")
])

# Let's make a search function
def search_places_sparse_metadata(query, sparse_model, embeddings_matrix, top_k=5):
    # Embed the query (FastEmbed sparse model)
    query_sparse = list(sparse_model.embed([query]))[0]
    query_dense = sparse_to_dense(query_sparse, dim=30315).reshape(1, -1)

    # Compute cosine similarities
    similarities = cosine_similarity(query_dense, embeddings_matrix)[0]

    # Fast top-k retrieval
    top_indices = np.argpartition(-similarities, top_k)[:top_k]
    top_indices = top_indices[np.argsort(similarities[top_indices])[::-1]]

    # Prepare results
    results = []
    for idx in top_indices:
        row = merge_df.iloc[idx]
        results.append({
            'Place Name': row['name'],
            'Neighborhood': row['neighborhood'],
            'Tags': row['tags'],
            'Description': row['short_description'],
            'Similarity': similarities[idx]
        })

    return results

# Let's try a query and output the 5 nearest embeddings
search_results = search_places_sparse_metadata("dance-y bars that have disco balls", sparse_model, sparse_embeddings_dense, top_k=5)

# Output the results
for result in search_results:
    print("------------------------------")
    print(f"Place Name        : {result['Place Name']}")
    print(f"Neighborhood      : {result['Neighborhood']}")
    print(f"Tags              : {result['Tags']}")
    print(f"Description       : {result['Description']}")
    print(f"Cosine Similarity : {result['Similarity']:.4f}")

"""Now that we have our three similarity search functions, let's final make our hybrid search function! First, we need to normalize FAISS distance."""

from sklearn.preprocessing import MinMaxScaler

# helper to normalize scores between 0 and 1
def normalize_scores(scores):
    scores = np.array(scores).reshape(-1, 1)
    scaler = MinMaxScaler()
    return scaler.fit_transform(scores).flatten()

# hybrid search function
# Full hybrid search function
def hybrid_search(query, metadata_index, image_index, sparse_embeddings,
                  sparse_model, metadata_model, processor, clip_model, top_k=5,
                  weight_dense=0.4, weight_sparse=0.3, weight_image=0.3):
    # Metadata (Dense)
    query_dense = metadata_model.encode([query], normalize_embeddings=True)[0].astype('float32').reshape(1, -1)
    distances_dense, indices_dense = metadata_index.search(query_dense, top_k)
    scores_dense = -distances_dense[0]  # Negative L2 distance → higher is better

    # Sparse Embeddings
    query_sparse = list(sparse_model.embed([query]))[0]
    query_sparse_dense = sparse_to_dense(query_sparse, dim=30315).reshape(1, -1)
    similarities_sparse = cosine_similarity(query_sparse_dense, sparse_embeddings)[0]
    top_indices_sparse = np.argpartition(-similarities_sparse, top_k)[:top_k]
    top_indices_sparse = top_indices_sparse[np.argsort(similarities_sparse[top_indices_sparse])[::-1]]
    scores_sparse = similarities_sparse[top_indices_sparse]

    # Image Embeddings
    inputs = processor(text=[query], return_tensors="pt", padding=True)
    query_image_embedding = clip_model.get_text_features(**inputs)
    query_image_embedding = query_image_embedding / query_image_embedding.norm(p=2, dim=-1, keepdim=True)
    query_image_embedding = query_image_embedding.detach().cpu().numpy().astype('float32').reshape(1, -1)
    distances_image, indices_image = image_index.search(query_image_embedding, top_k)
    scores_image = -distances_image[0]  # Negative L2 distance → higher is better

    # Normalize Scores
    norm_dense = normalize_scores(scores_dense)
    norm_sparse = normalize_scores(scores_sparse)
    norm_image = normalize_scores(scores_image)

    # Hybrid Scoring
    hybrid_scores = (weight_dense * norm_dense[:top_k] +
                     weight_sparse * norm_sparse[:top_k] +
                     weight_image * norm_image[:top_k])

    # Gather Results
    results = []
    for i in range(top_k):
        idx = indices_dense[0][i]  # Take from dense indices (you can adjust this logic if needed!)
        row = merge_df.iloc[idx]
        results.append({
            'Place Name': row['name'],
            'Neighborhood': row['neighborhood'],
            'Tags': row['tags'],
            'Description': row['short_description'],
            'Hybrid Score': hybrid_scores[i],
            'Dense Score': norm_dense[i],
            'Sparse Score': norm_sparse[i],
            'Image Score': norm_image[i]
        })

    results = [res for res in results if res['Hybrid Score'] > 0.1]

    # Sort Final Output
    results = sorted(results, key=lambda x: x['Hybrid Score'], reverse=True)

    return results

user_query = "dance-y bars that have disco balls"

results = hybrid_search(
    query=user_query,
    metadata_index=index_dense_metadata,       # Your dense FAISS index
    image_index=index_image,                   # Your image FAISS index
    sparse_embeddings=sparse_embeddings_dense, # Your dense-converted sparse embeddings
    metadata_model=metadata_model,
    sparse_model=sparse_model,                 # Dense text embedding model (MiniLM)
    processor=processor,                       # CLIP processor
    clip_model=clip_model,                         # CLIP model
)

for res in results:
    print("-------------------------")
    print(f"Place Name       : {res['Place Name']}")
    print(f"Neighborhood     : {res['Neighborhood']}")
    print(f"Tags             : {res['Tags']}")
    print(f"Description      : {res['Description']}")
    print(f"Hybrid Score     : {res['Hybrid Score']:.4f}")
    print(f"Dense Score      : {res['Dense Score']:.4f}")
    print(f"Image Score      : {res['Image Score']:.4f}")
    print(f"Sparse Score     : {res['Sparse Score']:.4f}")

"""## (BONUS) Query Categorization and Generation"""

import spacy
import re

# Load spacy NER and sentence embedding model
nlp = spacy.load("en_core_web_sm")
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # or whatever model you used

# Keyword sets for weather and activities
CATEGORY_ANCHORS = {
    "eat": "restaurants, bistros, brunch, cafes, dining",
    "drink": "bars, cocktails, rooftop, pubs",
    "study": "libraries, quiet, study",
    "dance": "dance, clubs, nightclubs, places to party",
    "date": "romantic, date, venues, cozy",
    "coffee": "coffee, cafe, breakfast",
    "chill": "relaxing, lounges, casual, open, chill",
    "outdoor": "outdoor, rooftops, patios, park, picnic",
    "work": "co-working, WiFi, library, charger"
}

WEATHER_KEYWORDS = {
    "sunny", "rainy", "cozy", "warm", "cold", "chilly", "stormy",
    "outdoor", "indoor", "rooftop"
}

def detect_keywords(text, keyword_set):
    detected = []
    text = text.lower()
    for keyword in keyword_set:
        if keyword in text:
            detected.append(keyword)
    return detected if detected else None

def semantic_category_detection(user_query, category_anchors, model):
    query_emb = model.encode(user_query, normalize_embeddings=True)

    category_names = []
    category_descs = []
    for category, desc in category_anchors.items():
        category_names.append(category)
        category_descs.append(desc)

    anchor_embs = model.encode(category_descs, normalize_embeddings=True)

    # Compute cosine similarity
    sims = (query_emb @ anchor_embs.T).tolist()  # or use sklearn cosine_similarity

    detected_categories = []
    for idx, score in enumerate(sims):
        if score > 0.4:  # You can tune this threshold
            detected_categories.append(category_names[idx])

    return detected_categories


def expand_query_with_llm(query):
    # Preprocessing: Lowercase and remove extra spaces
    query = query.lower().strip()
    query = re.sub(' +', ' ', query)

    expansions = {
        "study": "quiet cafes, libraries, study lounges",
        "dance": "dance clubs, vibrant party venues",
        "coffee": "coffee shops, cafes, breakfast spots",
    }

    for key, expanded in expansions.items():
        # Check for key presence using regex for flexibility
        if re.search(r'\b' + key + r'\b', query):
            return f"{query} ({expanded})"
    return query


def process_user_query(user_query):
    result = {}

    # Embed the query
    result['embedding'] = embedding_model.encode(user_query, normalize_embeddings=True).tolist()

    # NER for location
    doc = nlp(user_query)
    locations = [ent.text for ent in doc.ents if ent.label_ in {"GPE", "LOC", "FAC"}]
    result['detected_location'] = locations[0] if locations else None

    result['detected_activity'] = semantic_category_detection(user_query, CATEGORY_ANCHORS, embedding_model)
    result['detected_weather'] = detect_keywords(user_query, WEATHER_KEYWORDS)

    # Optional: Expand vague queries
    result['expanded_prompt'] = expand_query_with_llm(user_query)

    return result


def process_queries_from_file(filepath):
    all_results = []

    # Read all queries from the file
    with open(filepath, 'r', encoding='utf-8') as f:
        queries = [line.strip() for line in f if line.strip()]

    # Process each query
    for query in queries:
        result = process_user_query(query)
        result['original_query'] = query  # keep the original too
        all_results.append(result)

    df = pd.DataFrame(all_results)
    return df

filepath = 'manual_generated_queries.txt'
query_df = process_queries_from_file(filepath)

print(query_df.head())