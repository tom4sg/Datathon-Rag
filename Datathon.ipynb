{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nmHykLagB2e7",
        "xUr8ufSfHSHM",
        "BQzwCV35KnAD",
        "WyhM1mk-HbJy",
        "-UbN_UK4IE6T",
        "D43N0b8sa_8r"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Datathon 2025 RAG Solution"
      ],
      "metadata": {
        "id": "nmHykLagB2e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingestion"
      ],
      "metadata": {
        "id": "xUr8ufSfHSHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first import relevant packages, take a look at our data, and create a master dataframe."
      ],
      "metadata": {
        "id": "uDHC0aF4ZTzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "mVZ5V18dHOGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "media_df = pd.read_csv('P2/media.csv')\n",
        "places_p2_df = pd.read_csv('P2/places.csv')\n",
        "reviews_df = pd.read_csv('P2/reviews.csv')"
      ],
      "metadata": {
        "id": "jIn0f0piCGkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"P2 Media preview: {media_df.head}\\n\")\n",
        "print(f\"P2 Places preview: {places_p2_df.head}\\n\")\n",
        "print(f\"P2 Reviews preview: {reviews_df.head}\\n\")"
      ],
      "metadata": {
        "id": "sqs4G-oSNhSg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggregate reviews corresponding to a given place_id into a list of strings. Do the same for media, then merge both into larger df."
      ],
      "metadata": {
        "id": "vbV5aAv6EVPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df = reviews_df.drop_duplicates(subset=['place_id', 'review_text'])\n",
        "media_df = media_df.drop_duplicates(subset=['place_id', 'media_url'])\n",
        "\n",
        "reviews_agg = (\n",
        "    reviews_df\n",
        "    .groupby('place_id')['review_text']\n",
        "    .apply(list)\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "media_agg = (\n",
        "    media_df\n",
        "    .groupby('place_id')['media_url']\n",
        "    .apply(list)\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "merge_df = pd.merge(places_p2_df, media_agg, on='place_id', how='inner')\n",
        "merge_df = pd.merge(merge_df, reviews_agg, on='place_id', how='inner')\n",
        "merge_df = merge_df.drop_duplicates(subset='place_id').reset_index(drop=True)\n",
        "merge_df.head()"
      ],
      "metadata": {
        "id": "jlvKugyQNY_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's concatenate reviews associated with a given place_id in preparation for embedding. Hopefully this can help with vibier searches."
      ],
      "metadata": {
        "id": "ABoPHFQiQhOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def concat_reviews(series):\n",
        "    \"\"\"Join all review texts in the group into a single string.\"\"\"\n",
        "    return ' '.join(series.astype(str))\n",
        "\n",
        "# Group reviews and create the all_reviews column\n",
        "agg_reviews = (\n",
        "    reviews_df\n",
        "    .groupby('place_id')['review_text']\n",
        "    .apply(concat_reviews)\n",
        "    .reset_index(name='concat_reviews')\n",
        ")\n",
        "\n",
        "# Merge the concatenated reviews into merge_df\n",
        "merge_df = merge_df.merge(\n",
        "    agg_reviews,\n",
        "    how='left',\n",
        "    left_on='place_id',\n",
        "    right_on='place_id'\n",
        ")"
      ],
      "metadata": {
        "id": "xzwHTgJpRPQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write a function to prepare available structured data for semantic embedding, tack on the concatenated reviews and add it to our df."
      ],
      "metadata": {
        "id": "TXGgzRbQGRUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combining_text(row):\n",
        "    name = str(row.get('name', ''))\n",
        "    neighborhood = str(row.get('neighborhood', ''))\n",
        "    tags = str(row.get('tags', ''))\n",
        "    short_description = str(row.get('short_description', ''))\n",
        "    emojis = str(row.get('emojis', ''))\n",
        "    reviews = str(row.get('concat_reviews', ''))\n",
        "\n",
        "    combined_text = (\n",
        "        f\"Name: {name}. \"\n",
        "        f\"Neighborhood: {neighborhood}. \"\n",
        "        f\"Tags: {tags}. \"\n",
        "        f\"Description: {short_description}. \"\n",
        "        f\"Emojis: {emojis}.\"\n",
        "        f\"User Reviews: {reviews}.\"\n",
        "    )\n",
        "    return combined_text\n",
        "\n",
        "merge_df['combined_text'] = merge_df.apply(combining_text, axis=1)"
      ],
      "metadata": {
        "id": "Gy5YgKDTLm6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, now let's actually start embedding our data. Cross-Encoders will be too computationally heavy for this. Instead will use bi-encoders such as sentence_transformers and CLIP for seemantic and multimodal embeddings respectively."
      ],
      "metadata": {
        "id": "b13bfa8cQVcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metadata Text Embeddings (Dense + Sparse)"
      ],
      "metadata": {
        "id": "BQzwCV35KnAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eventually, we want a hybrid search which requires is a combination of dense, sparse, and multimodal embeddings. Let's start with dense."
      ],
      "metadata": {
        "id": "FipmpqyqafSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tqdm and MiniLM model(dense semantic text embedding)\n",
        "from tqdm.auto import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "metadata_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "AAUX2eprQdpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = []\n",
        "batch_size = 32\n",
        "\n",
        "for i in tqdm(range(0, len(merge_df), batch_size), desc=\"Generating embeddings from dataset\"):\n",
        "    batch = merge_df['combined_text'].iloc[i: i + batch_size].tolist()\n",
        "    batch_embeddings = metadata_model.encode(batch, normalize_embeddings=True)\n",
        "    embeddings.append(batch_embeddings)\n",
        "\n",
        "embeddings = np.vstack(embeddings)\n",
        "\n",
        "# Save in Dataframe\n",
        "merge_df['dense_metadata_embedding'] = embeddings.tolist()"
      ],
      "metadata": {
        "id": "v6ANKxlcIK55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, now let's try a sparse text embedding model from fastembed. Hopefully with the hybrid, we can pick up explicit meaning as well as implied."
      ],
      "metadata": {
        "id": "k4GB6MBidMtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastembed"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QpS8hiYygg2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastembed import SparseTextEmbedding\n",
        "sparse_model = SparseTextEmbedding(model_name=\"prithivida/Splade_PP_en_v1\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jgti9dRxgZvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time to embed!"
      ],
      "metadata": {
        "id": "0fRLY4gFgz2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sparse_embeddings = []\n",
        "\n",
        "for i in tqdm(range(0, len(merge_df), batch_size), desc=\"Generating sparse embeddings\"):\n",
        "    batch = merge_df['combined_text'].iloc[i: i + batch_size].tolist()\n",
        "    batch_embeddings = list(sparse_model.embed(batch))\n",
        "    sparse_embeddings.extend(batch_embeddings)\n",
        "\n",
        "merge_df['sparse_metadata_embedding'] = sparse_embeddings"
      ],
      "metadata": {
        "id": "YGUvV4Kngzb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's save it real quick."
      ],
      "metadata": {
        "id": "hqjfKikdnqgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib"
      ],
      "metadata": {
        "id": "_uHvMALgnnu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DataFrame\n",
        "joblib.dump(merge_df, 'merge_dense_and_sparse_df.joblib')"
      ],
      "metadata": {
        "id": "gDUTeANUnk4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Media (Image) Embeddings"
      ],
      "metadata": {
        "id": "WyhM1mk-HbJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because there are over 30,000 media_urls, We opted for batch processing 1 image per place_id, reducing the image's resolution, and then embedding it using a Hugging Face CLIP model for multimodal embedding in another notebook. The result is the image_embeddings_sorted.csv that we can simply read in as a df. Ideally, with more time we could embed several/all images corresponding to a place_id, and then take their arithmetic mean for a more generally representative embedding per place_id."
      ],
      "metadata": {
        "id": "d0AD7aIn9OfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_df = pd.read_csv('image_embeddings_sorted.csv')"
      ],
      "metadata": {
        "id": "4qR6h-y2KVXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, even though we already embedded the images in another file, let's import and load the CLIP model hear to use for embedding queries in the future."
      ],
      "metadata": {
        "id": "cnn7DfaGKmxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedded 1 image for each location using CLIPProcessor, CLIPModel in another ipynb\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# Load the CLIP model and processor\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "g3v6T6QE5gKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector Similarity Search + Hybrid Search Model"
      ],
      "metadata": {
        "id": "8DMYDVBO4gk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use Facebook AI Similarity Search (FAISS) for efficient vector similarity search on our:\n",
        "- Metadata Embeddings (Dense)\n",
        "- Media Embeddings (Image)\n",
        "\n",
        "And, let's use Cosine Similarity for our sparse model."
      ],
      "metadata": {
        "id": "frKcyrazC1nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "vm4gQjwfLeV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "mTcFjYd5MOPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metadata FAISS (Dense)"
      ],
      "metadata": {
        "id": "-UbN_UK4IE6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get metadata embeddings\n",
        "all_dense_embeddings = np.array(merge_df['dense_metadata_embedding'].tolist(), dtype='float32')\n",
        "\n",
        "# Determine embedding dimension\n",
        "d = all_dense_embeddings.shape[1]\n",
        "\n",
        "# Create FAISS index (L2 distance)\n",
        "index_dense_metadata = faiss.IndexFlatL2(d)\n",
        "index_dense_metadata.add(all_dense_embeddings)\n",
        "\n",
        "print(f\"Built FAISS dense metadata index with {index_dense_metadata.ntotal} vectors of dimension {d}.\")\n",
        "\n",
        "# Let's make a search function\n",
        "def search_places_dense_metadata(query, index=index_dense_metadata, top_k=5):\n",
        "    query_embedding = metadata_model.encode([query], normalize_embeddings=True)[0].astype('float32').reshape(1, -1)\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    results = []\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        row = merge_df.iloc[idx]\n",
        "        results.append({\n",
        "            'Place Name': row['name'],\n",
        "            'Neighborhood': row['neighborhood'],\n",
        "            'Tags': row['tags'],\n",
        "            'Description': row['short_description'],\n",
        "            'Distance': distances[0][i]\n",
        "        })\n",
        "    return results"
      ],
      "metadata": {
        "id": "NfYcQivnIKaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try a query and output the 5 nearest embeddings\n",
        "query = \"where to drink a matcha\"\n",
        "search_results = search_places_dense_metadata(query, index_dense_metadata, top_k=5)\n",
        "\n",
        "for result in search_results:\n",
        "    print(\"------------------------------\")\n",
        "    print(f\"Place Name   : {result['Place Name']}\")\n",
        "    print(f\"Neighborhood : {result['Neighborhood']}\")\n",
        "    print(f\"Tags         : {result['Tags']}\")\n",
        "    print(f\"Description  : {result['Description']}\")\n",
        "    print(f\"L2 Distance  : {result['Distance']:.4f}\")"
      ],
      "metadata": {
        "id": "T_41hx_N7L51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image FAISS"
      ],
      "metadata": {
        "id": "hUpUuDDWN-EH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract embedding columns (assumes embed_0 to embed_511)\n",
        "embedding_cols = [col for col in image_df.columns if col.startswith('embed_')]\n",
        "all_embeddings = image_df[embedding_cols].to_numpy().astype('float32')\n",
        "\n",
        "# Determine embedding dimension\n",
        "d = all_embeddings.shape[1]\n",
        "\n",
        "# Build FAISS index\n",
        "index_image = faiss.IndexFlatL2(d)\n",
        "index_image.add(all_embeddings)\n",
        "\n",
        "print(f\"Built FAISS image index with {index_image.ntotal} vectors of dimension {d}.\")\n",
        "\n",
        "# Let's make a search function\n",
        "def search_places_image(query, index=index_image, top_k=5):\n",
        "    # Encode the text query using CLIP text encoder\n",
        "    inputs = processor(text=[query], return_tensors=\"pt\", padding=True)\n",
        "    query_embedding = clip_model.get_text_features(**inputs)\n",
        "    query_embedding = query_embedding / query_embedding.norm(p=2, dim=-1, keepdim=True)\n",
        "    query_embedding = query_embedding.detach().cpu().numpy().astype('float32').reshape(1, -1)\n",
        "\n",
        "    # Perform FAISS search\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    # Gather results from image_df (which holds the place info)\n",
        "    results = []\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        row = merge_df.iloc[idx]\n",
        "        results.append({\n",
        "            'Place Name': row['name'],\n",
        "            'Neighborhood': row['neighborhood'],\n",
        "            'Tags': row['tags'],\n",
        "            'Description': row['short_description'],\n",
        "            'Distance': distances[0][i]\n",
        "        })\n",
        "    return results"
      ],
      "metadata": {
        "id": "1OsT-8TQ4cMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try a query and output the 5 nearest embeddings\n",
        "search_results = search_places_image(\"Something to do on a gloomy day\", index_image, top_k=5)\n",
        "\n",
        "for result in search_results:\n",
        "    print(\"------------------------------\")\n",
        "    print(f\"Place Name   : {result['Place Name']}\")\n",
        "    print(f\"Neighborhood : {result['Neighborhood']}\")\n",
        "    print(f\"Tags         : {result['Tags']}\")\n",
        "    print(f\"Description  : {result['Description']}\")\n",
        "    print(f\"L2 Distance  : {result['Distance']:.4f}\")"
      ],
      "metadata": {
        "id": "Ge6u7_Ae5P-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metadata Cosine Similarity (Sparse)"
      ],
      "metadata": {
        "id": "WtnFE_0Ssdur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_to_dense(sparse_embedding, dim=30315):\n",
        "    dense_vec = np.zeros(dim, dtype=np.float32)\n",
        "    dense_vec[sparse_embedding.indices] = sparse_embedding.values\n",
        "    return dense_vec\n",
        "\n",
        "print(\"Converting sparse embeddings to dense matrix...\")\n",
        "sparse_embeddings_dense = np.vstack([\n",
        "    sparse_to_dense(embedding, dim=30315)\n",
        "    for embedding in tqdm(merge_df['sparse_metadata_embedding'], desc=\"Converting embeddings\")\n",
        "])\n",
        "\n",
        "# Let's make a search function\n",
        "def search_places_sparse_metadata(query, sparse_model, embeddings_matrix, top_k=5):\n",
        "    # Embed the query (FastEmbed sparse model)\n",
        "    query_sparse = list(sparse_model.embed([query]))[0]\n",
        "    query_dense = sparse_to_dense(query_sparse, dim=30315).reshape(1, -1)\n",
        "\n",
        "    # Compute cosine similarities\n",
        "    similarities = cosine_similarity(query_dense, embeddings_matrix)[0]\n",
        "\n",
        "    # Fast top-k retrieval\n",
        "    top_indices = np.argpartition(-similarities, top_k)[:top_k]\n",
        "    top_indices = top_indices[np.argsort(similarities[top_indices])[::-1]]\n",
        "\n",
        "    # Prepare results\n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        row = merge_df.iloc[idx]\n",
        "        results.append({\n",
        "            'Place Name': row['name'],\n",
        "            'Neighborhood': row['neighborhood'],\n",
        "            'Tags': row['tags'],\n",
        "            'Description': row['short_description'],\n",
        "            'Similarity': similarities[idx]\n",
        "        })\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "1dbHnfxfsWQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try a query and output the 5 nearest embeddings\n",
        "search_results = search_places_sparse_metadata(\"dance-y bars that have disco balls\", sparse_model, sparse_embeddings_dense, top_k=5)\n",
        "\n",
        "# Output the results\n",
        "for result in search_results:\n",
        "    print(\"------------------------------\")\n",
        "    print(f\"Place Name        : {result['Place Name']}\")\n",
        "    print(f\"Neighborhood      : {result['Neighborhood']}\")\n",
        "    print(f\"Tags              : {result['Tags']}\")\n",
        "    print(f\"Description       : {result['Description']}\")\n",
        "    print(f\"Cosine Similarity : {result['Similarity']:.4f}\")"
      ],
      "metadata": {
        "id": "iQJL63Omv0XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our three similarity search functions, let's final make our hybrid search function! First, we need to normalize FAISS distance."
      ],
      "metadata": {
        "id": "M4eh2osw1k5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "td5HgszM41ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper to normalize scores between 0 and 1\n",
        "def normalize_scores(scores):\n",
        "    scores = np.array(scores).reshape(-1, 1)\n",
        "    scaler = MinMaxScaler()\n",
        "    return scaler.fit_transform(scores).flatten()"
      ],
      "metadata": {
        "id": "ozwtPt_W2mOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hybrid search function\n",
        "# Full hybrid search function\n",
        "def hybrid_search(query, metadata_index, image_index, sparse_embeddings,\n",
        "                  sparse_model, metadata_model, processor, clip_model, top_k=5,\n",
        "                  weight_dense=0.4, weight_sparse=0.3, weight_image=0.3):\n",
        "    # Metadata (Dense)\n",
        "    query_dense = metadata_model.encode([query], normalize_embeddings=True)[0].astype('float32').reshape(1, -1)\n",
        "    distances_dense, indices_dense = metadata_index.search(query_dense, top_k)\n",
        "    scores_dense = -distances_dense[0]  # Negative L2 distance → higher is better\n",
        "\n",
        "    # Sparse Embeddings\n",
        "    query_sparse = list(sparse_model.embed([query]))[0]\n",
        "    query_sparse_dense = sparse_to_dense(query_sparse, dim=30315).reshape(1, -1)\n",
        "    similarities_sparse = cosine_similarity(query_sparse_dense, sparse_embeddings)[0]\n",
        "    top_indices_sparse = np.argpartition(-similarities_sparse, top_k)[:top_k]\n",
        "    top_indices_sparse = top_indices_sparse[np.argsort(similarities_sparse[top_indices_sparse])[::-1]]\n",
        "    scores_sparse = similarities_sparse[top_indices_sparse]\n",
        "\n",
        "    # Image Embeddings\n",
        "    inputs = processor(text=[query], return_tensors=\"pt\", padding=True)\n",
        "    query_image_embedding = clip_model.get_text_features(**inputs)\n",
        "    query_image_embedding = query_image_embedding / query_image_embedding.norm(p=2, dim=-1, keepdim=True)\n",
        "    query_image_embedding = query_image_embedding.detach().cpu().numpy().astype('float32').reshape(1, -1)\n",
        "    distances_image, indices_image = image_index.search(query_image_embedding, top_k)\n",
        "    scores_image = -distances_image[0]  # Negative L2 distance → higher is better\n",
        "\n",
        "    # Normalize Scores\n",
        "    norm_dense = normalize_scores(scores_dense)\n",
        "    norm_sparse = normalize_scores(scores_sparse)\n",
        "    norm_image = normalize_scores(scores_image)\n",
        "\n",
        "    # Hybrid Scoring\n",
        "    hybrid_scores = (weight_dense * norm_dense[:top_k] +\n",
        "                     weight_sparse * norm_sparse[:top_k] +\n",
        "                     weight_image * norm_image[:top_k])\n",
        "\n",
        "    # Gather Results\n",
        "    results = []\n",
        "    for i in range(top_k):\n",
        "        idx = indices_dense[0][i]  # Take from dense indices\n",
        "        row = merge_df.iloc[idx]\n",
        "        results.append({\n",
        "            'Place Name': row['name'],\n",
        "            'Neighborhood': row['neighborhood'],\n",
        "            'Tags': row['tags'],\n",
        "            'Description': row['short_description'],\n",
        "            'Hybrid Score': hybrid_scores[i],\n",
        "            'Dense Score': norm_dense[i],\n",
        "            'Sparse Score': norm_sparse[i],\n",
        "            'Image Score': norm_image[i]\n",
        "        })\n",
        "\n",
        "    results = [res for res in results if res['Hybrid Score'] > 0.1]\n",
        "\n",
        "    # Sort Final Output\n",
        "    results = sorted(results, key=lambda x: x['Hybrid Score'], reverse=True)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "7tLx5fndBWtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"what to do this weekend\"\n",
        "\n",
        "results = hybrid_search(\n",
        "    query=user_query,\n",
        "    metadata_index=index_dense_metadata,       # dense FAISS index\n",
        "    image_index=index_image,                   # image FAISS index\n",
        "    sparse_embeddings=sparse_embeddings_dense, # dense-converted sparse embeddings\n",
        "    metadata_model=metadata_model,\n",
        "    sparse_model=sparse_model,                 # Dense text embedding model (MiniLM)\n",
        "    processor=processor,                       # CLIP processor\n",
        "    clip_model=clip_model,                         # CLIP model\n",
        ")\n",
        "\n",
        "for res in results:\n",
        "    print(\"-------------------------\")\n",
        "    print(f\"Place Name       : {res['Place Name']}\")\n",
        "    print(f\"Neighborhood     : {res['Neighborhood']}\")\n",
        "    print(f\"Tags             : {res['Tags']}\")\n",
        "    print(f\"Description      : {res['Description']}\")\n",
        "    print(f\"Hybrid Score     : {res['Hybrid Score']:.4f}\")\n",
        "    print(f\"Dense Score      : {res['Dense Score']:.4f}\")\n",
        "    print(f\"Image Score      : {res['Image Score']:.4f}\")\n",
        "    print(f\"Sparse Score     : {res['Sparse Score']:.4f}\")"
      ],
      "metadata": {
        "id": "d3OobQG2287A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (BONUS) Query Categorization and Generation"
      ],
      "metadata": {
        "id": "D43N0b8sa_8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import re\n",
        "\n",
        "# Load spacy NER and sentence embedding model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # or whatever model you used\n",
        "\n",
        "# Keyword sets for weather and activities\n",
        "CATEGORY_ANCHORS = {\n",
        "    \"eat\": \"restaurants, bistros, brunch, cafes, dining\",\n",
        "    \"drink\": \"bars, cocktails, rooftop, pubs\",\n",
        "    \"study\": \"libraries, quiet, study\",\n",
        "    \"dance\": \"dance, clubs, nightclubs, places to party\",\n",
        "    \"date\": \"romantic, date, venues, cozy\",\n",
        "    \"coffee\": \"coffee, cafe, breakfast\",\n",
        "    \"chill\": \"relaxing, lounges, casual, open, chill\",\n",
        "    \"outdoor\": \"outdoor, rooftops, patios, park, picnic\",\n",
        "    \"work\": \"co-working, WiFi, library, charger\"\n",
        "}\n",
        "\n",
        "WEATHER_KEYWORDS = {\n",
        "    \"sunny\", \"rainy\", \"cozy\", \"warm\", \"cold\", \"chilly\", \"stormy\",\n",
        "    \"outdoor\", \"indoor\", \"rooftop\"\n",
        "}\n",
        "\n",
        "def detect_keywords(text, keyword_set):\n",
        "    detected = []\n",
        "    text = text.lower()\n",
        "    for keyword in keyword_set:\n",
        "        if keyword in text:\n",
        "            detected.append(keyword)\n",
        "    return detected if detected else None\n",
        "\n",
        "def semantic_category_detection(user_query, category_anchors, model):\n",
        "    query_emb = model.encode(user_query, normalize_embeddings=True)\n",
        "\n",
        "    category_names = []\n",
        "    category_descs = []\n",
        "    for category, desc in category_anchors.items():\n",
        "        category_names.append(category)\n",
        "        category_descs.append(desc)\n",
        "\n",
        "    anchor_embs = model.encode(category_descs, normalize_embeddings=True)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    sims = (query_emb @ anchor_embs.T).tolist()  # or use sklearn cosine_similarity\n",
        "\n",
        "    detected_categories = []\n",
        "    for idx, score in enumerate(sims):\n",
        "        if score > 0.4:  # You can tune this threshold\n",
        "            detected_categories.append(category_names[idx])\n",
        "\n",
        "    return detected_categories\n",
        "\n",
        "\n",
        "def expand_query_with_llm(query):\n",
        "    # Preprocessing: Lowercase and remove extra spaces\n",
        "    query = query.lower().strip()\n",
        "    query = re.sub(' +', ' ', query)\n",
        "\n",
        "    expansions = {\n",
        "        \"study\": \"quiet cafes, libraries, study lounges\",\n",
        "        \"dance\": \"dance clubs, vibrant party venues\",\n",
        "        \"coffee\": \"coffee shops, cafes, breakfast spots\",\n",
        "    }\n",
        "\n",
        "    for key, expanded in expansions.items():\n",
        "        # Check for key presence using regex for flexibility\n",
        "        if re.search(r'\\b' + key + r'\\b', query):\n",
        "            return f\"{query} ({expanded})\"\n",
        "    return query\n",
        "\n",
        "\n",
        "def process_user_query(user_query):\n",
        "    result = {}\n",
        "\n",
        "    # Embed the query\n",
        "    result['embedding'] = embedding_model.encode(user_query, normalize_embeddings=True).tolist()\n",
        "\n",
        "    # NER for location\n",
        "    doc = nlp(user_query)\n",
        "    locations = [ent.text for ent in doc.ents if ent.label_ in {\"GPE\", \"LOC\", \"FAC\"}]\n",
        "    result['detected_location'] = locations[0] if locations else None\n",
        "\n",
        "    result['detected_activity'] = semantic_category_detection(user_query, CATEGORY_ANCHORS, embedding_model)\n",
        "    result['detected_weather'] = detect_keywords(user_query, WEATHER_KEYWORDS)\n",
        "\n",
        "    # Optional: Expand vague queries\n",
        "    result['expanded_prompt'] = expand_query_with_llm(user_query)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def process_queries_from_file(filepath):\n",
        "    all_results = []\n",
        "\n",
        "    # Read all queries from the file\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        queries = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "    # Process each query\n",
        "    for query in queries:\n",
        "        result = process_user_query(query)\n",
        "        result['original_query'] = query  # keep the original too\n",
        "        all_results.append(result)\n",
        "\n",
        "    df = pd.DataFrame(all_results)\n",
        "    return df"
      ],
      "metadata": {
        "id": "yUYOWEyHbHFO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
